{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1ee9a3-8851-439b-92d4-11f9cedf9e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS bucket: sagemaker-us-east-1-348052051973\n",
      "AWS account: 348052051973\n",
      "AWS region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f\"AWS bucket: {bucket}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: {account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0ddc66b-a805-4674-bfa2-d1e8a0df1dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_s3 = {\n",
    "    'model': sagemaker.inputs.TrainingInput(s3_data='s3://nlp-348052051973/model/llama-7b-hf/'),\n",
    "    #'model': sagemaker.inputs.TrainingInput(s3_data='s3://nlp-sagemakers/models/llama-hf-13b', content_type='application/x-sagemaker-model'),\n",
    "    'data': sagemaker.inputs.TrainingInput(s3_data='s3://nlp-348052051973/data')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebbb6a52-3214-4e62-9f1d-500fcae5000f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "034cb21b-08d2-4858-87c6-9235605985ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b78c65c8-ba2f-4b80-b6f1-a538416b7862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = '348052051973.dkr.ecr.us-east-1.amazonaws.com/alpaca:latest'\n",
    "job_name = 'nlp-sft-persona-7b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "add0a2be-f47a-4d72-9c92-a31a20dd6a18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "        {'Name': 'loss', 'Regex': \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'learning_rate', 'Regex': \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b482f236-01ef-4d3b-a6f8-0f7645ccd02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_estimator = PyTorch(base_job_name=job_name,\n",
    "                            source_dir=\"./stanford_alpaca\",\n",
    "                            entry_point=\"train-llama7B.sh\",  #  the entry point that launches the training script with options\n",
    "                            role=role,\n",
    "                            image_uri=image_uri,\n",
    "                            instance_type='ml.p4d.24xlarge',\n",
    "                            instance_count=1,\n",
    "                            metric_definitions=metric_definitions,\n",
    "                            # hyperparameters=hyperparameters,\n",
    "                            #distribution=distribution\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "028916d6-bb00-4984-bda5-eb4812f6e3f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: nlp-sft-persona-7b-2023-04-10-12-58-28-129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 12:58:33 Starting - Starting the training job...\n",
      "2023-04-10 12:59:09 Starting - Preparing the instances for training............\n",
      "2023-04-10 13:00:59 Downloading - Downloading input data......\n",
      "2023-04-10 13:01:49 Training - Downloading the training image...............\n",
      "2023-04-10 13:04:31 Training - Training image download completed. Training in progress.....\u001b[34m2023-04-10 13:05:20,367 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-10 13:05:20,467 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-10 13:05:20,575 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-04-10 13:05:20,583 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"data\": \"/opt/ml/input/data/data\",\n",
      "        \"model\": \"/opt/ml/input/data/model\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"data\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"nlp-sft-persona-7b-2023-04-10-12-58-28-129\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-348052051973/nlp-sft-persona-7b-2023-04-10-12-58-28-129/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-llama7B.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-llama7B.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-llama7B.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"data\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"data\",\"model\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-llama7B.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-348052051973/nlp-sft-persona-7b-2023-04-10-12-58-28-129/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"data\":\"/opt/ml/input/data/data\",\"model\":\"/opt/ml/input/data/model\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"data\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"nlp-sft-persona-7b-2023-04-10-12-58-28-129\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-348052051973/nlp-sft-persona-7b-2023-04-10-12-58-28-129/source/sourcedir.tar.gz\",\"module_name\":\"train-llama7B.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-llama7B.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_DATA=/opt/ml/input/data/data\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python310.zip:/usr/local/lib/python3.10:/usr/local/lib/python3.10/lib-dynload:/usr/local/lib/python3.10/site-packages:/data/sm-alpaca/transformers-zphan/src\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./train-llama7B.sh \"\u001b[0m\n",
      "\u001b[34m2023-04-10 13:05:20,583 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2023-04-10 13:05:20,583 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mWARNING:root:Version 1\u001b[0m\n",
      "\u001b[34mWARNING:root:Version 1\u001b[0m\n",
      "\u001b[34mWARNING:root:Version 1\u001b[0m\n",
      "\u001b[34mWARNING:root:Version 1\u001b[0m\n",
      "\u001b[34mWARNING:root:Version 1\u001b[0m\n",
      "\u001b[34mWARNING:root:Version 1\u001b[0m\n",
      "\u001b[34mWARNING:root:Version 1\u001b[0m\n",
      "\u001b[34mWARNING:root:Version 1\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34mUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.25it/s]#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.14it/s]#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.20it/s]#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.18it/s]#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.13it/s]#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:14,  2.14it/s]#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.13it/s]#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:13,  2.26it/s]#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:15,  2.08it/s]#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.17it/s]#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.18it/s]#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.17it/s]#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.11it/s]#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.12it/s]#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.12it/s]#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.26it/s]#015Loading checkpoint shards:   6%|▌         | 2/33 [00:00<00:14,  2.09it/s]#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.17it/s]#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.17it/s]#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:13,  2.16it/s]#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.10it/s]#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.11it/s]#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.11it/s]#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:12,  2.26it/s]#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:14,  2.09it/s]#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.17it/s]#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.16it/s]#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.16it/s]#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.09it/s]#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.10it/s]#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.10it/s]#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:12,  2.26it/s]#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:01<00:13,  2.09it/s]#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:12,  2.17it/s]#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.15it/s]#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.15it/s]#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.09it/s]#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.10it/s]#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.09it/s]#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:11,  2.26it/s]#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.17it/s]#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:13,  2.09it/s]#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.15it/s]#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.14it/s]#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.11it/s]#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.09it/s]#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:11,  2.26it/s]#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.09it/s]#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:11,  2.17it/s]#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:02<00:12,  2.08it/s]#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.14it/s]#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.14it/s]#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.26it/s]#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.10it/s]#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.08it/s]#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.08it/s]#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.17it/s]#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:12,  2.08it/s]#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.14it/s]#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.14it/s]#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:03<00:10,  2.26it/s]#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:11,  2.10it/s]#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.08it/s]#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.08it/s]#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.18it/s]#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.14it/s]#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.14it/s]#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:03<00:12,  2.07it/s]#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:10,  2.26it/s]#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.10it/s]#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.07it/s]#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.08it/s]#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:10,  2.17it/s]#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:10,  2.13it/s]#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:10,  2.13it/s]#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:11,  2.06it/s]#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:04<00:09,  2.26it/s]#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:10,  2.10it/s]#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.07it/s]#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.08it/s]#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.17it/s]#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.12it/s]#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.12it/s]#015Loading checkpoint shards:  30%|███       | 10/33 [00:04<00:11,  2.06it/s]#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:09,  2.27it/s]#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.09it/s]#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.07it/s]#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.07it/s]#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:09,  2.17it/s]#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:09,  2.12it/s]#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:09,  2.12it/s]#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:08,  2.27it/s]#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:10,  2.06it/s]#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.09it/s]#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.07it/s]#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.07it/s]#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:05<00:09,  2.16it/s]#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:08,  2.27it/s]#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.12it/s]#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.12it/s]#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:05<00:10,  2.06it/s]#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.09it/s]#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.06it/s]#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.07it/s]#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:08,  2.16it/s]#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:07,  2.27it/s]#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:08,  2.12it/s]#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:08,  2.12it/s]#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:09,  2.06it/s]#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.09it/s]#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.06it/s]#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.07it/s]#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:06<00:08,  2.16it/s]#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:07,  2.27it/s]#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.12it/s]#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.12it/s]#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:06<00:09,  2.06it/s]#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.09it/s]#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.06it/s]#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:07,  2.16it/s]#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.07it/s]#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:07,  2.27it/s]#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.12it/s]#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.12it/s]#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:07<00:08,  2.06it/s]#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.09it/s]#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.06it/s]#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:07,  2.16it/s]#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.07it/s]#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:07<00:06,  2.26it/s]#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:07,  2.12it/s]#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:07<00:07,  2.12it/s]#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:07<00:08,  2.07it/s]#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.09it/s]#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:06,  2.16it/s]#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:08<00:06,  2.26it/s]#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.06it/s]#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.07it/s]#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.12it/s]#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.12it/s]#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:08<00:07,  2.06it/s]#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.09it/s]#015Loading checkpoint shards:  61%|██████    | 20/33 [00:08<00:05,  2.26it/s]#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:08<00:06,  2.15it/s]#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.06it/s]#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.07it/s]#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:08<00:06,  2.12it/s]#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:08<00:06,  2.12it/s]#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:08<00:07,  2.06it/s]#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:09<00:05,  2.26it/s]#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.09it/s]#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.16it/s]#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.06it/s]#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.07it/s]#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.13it/s]#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.13it/s]#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:09<00:06,  2.06it/s]#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:09<00:04,  2.26it/s]#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.09it/s]#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:09<00:05,  2.16it/s]#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.06it/s]#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.07it/s]#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:09<00:05,  2.13it/s]#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:09<00:05,  2.13it/s]#015Loading checkpoint shards:  61%|██████    | 20/33 [00:09<00:06,  2.05it/s]#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.26it/s]#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.16it/s]#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.09it/s]#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.06it/s]#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.08it/s]#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.13it/s]#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.13it/s]#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:10<00:03,  2.26it/s]#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:10<00:05,  2.05it/s]#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.16it/s]#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.09it/s]#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.08it/s]#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.07it/s]#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.14it/s]#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.14it/s]#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.25it/s]#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:10<00:05,  2.02it/s]#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.16it/s]#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:10<00:04,  2.10it/s]#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.09it/s]#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.08it/s]#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.14it/s]#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.14it/s]#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:11<00:03,  2.24it/s]#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.16it/s]#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:11<00:04,  2.02it/s]#015Loading checkpoint\u001b[0m\n",
      "\u001b[34m shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.10it/s]#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.09it/s]#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.09it/s]#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.15it/s]#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.14it/s]#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:11<00:02,  2.23it/s]#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.15it/s]#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:11<00:03,  2.11it/s]#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:11<00:04,  2.01it/s]#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.09it/s]#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.09it/s]#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.15it/s]#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.15it/s]#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:12<00:02,  2.22it/s]#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.15it/s]#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.12it/s]#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:12<00:03,  2.00it/s]#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.09it/s]#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  2.09it/s]#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.16it/s]#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.16it/s]#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:12<00:01,  2.21it/s]#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:12<00:02,  2.14it/s]#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.13it/s]#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:12<00:03,  1.99it/s]#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.16it/s]#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.16it/s]#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:12<00:02,  2.08it/s]#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:02,  2.08it/s]#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:13<00:01,  2.20it/s]#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.14it/s]#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.13it/s]#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:13<00:03,  1.98it/s]#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.15it/s]#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.15it/s]#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.07it/s]#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  2.07it/s]#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:13<00:00,  2.20it/s]#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:13<00:01,  2.13it/s]#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.13it/s]#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.14it/s]#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.14it/s]#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:13<00:02,  1.97it/s]#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.06it/s]#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:13<00:01,  2.06it/s]#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:14<00:00,  2.20it/s]#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.13it/s]#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.13it/s]#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.13it/s]#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.13it/s]#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.05it/s]#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  2.05it/s]#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:14<00:02,  1.96it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:14<00:00,  2.01it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:14<00:00,  2.22it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:14<00:00,  2.13it/s]#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.13it/s]#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:14<00:00,  2.14it/s]#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:14<00:00,  2.14it/s]#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.06it/s]#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:14<00:00,  2.06it/s]#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:14<00:01,  1.97it/s]#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.11it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  1.98it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.14it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.06it/s]#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.06it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.00it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.12it/s]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.00it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.12it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:15<00:01,  1.98it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  1.99it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.09it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:15<00:00,  2.00it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.96it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.06it/s]\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  1.96it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:15<00:00,  2.07it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  1.90it/s]#015Loading checkpoint shards: 100%|██████████| 33/33 [00:16<00:00,  2.02it/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mWARNING:root:Loading data...\u001b[0m\n",
      "\u001b[34mWARNING:root:Formatting inputs...\u001b[0m\n",
      "\u001b[34mWARNING:root:Tokenizing inputs... This may take some time...\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/train.py\", line 240, in <module>\n",
      "    train()\n",
      "  File \"/opt/ml/code/train.py\", line 233, in train\n",
      "    trainer.train()\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1628, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1715, in _inner_training_loop\n",
      "    model = self._wrap_model(self.model_wrapped)\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1540, in _wrap_model\n",
      "    model = nn.parallel.DistributedDataParallel(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 688, in __init__\n",
      "    self._ddp_init_helper(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 825, in _ddp_init_helper\n",
      "    self.reducer = dist.Reducer(\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 3; 39.42 GiB total capacity; 37.86 GiB already allocated; 41.00 MiB free; 37.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/train.py\", line 240, in <module>\n",
      "    train()\n",
      "  File \"/opt/ml/code/train.py\", line 233, in train\n",
      "    trainer.train()\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1628, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1715, in _inner_training_loop\n",
      "    model = self._wrap_model(self.model_wrapped)\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1540, in _wrap_model\n",
      "    model = nn.parallel.DistributedDataParallel(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 688, in __init__\n",
      "    self._ddp_init_helper(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 825, in _ddp_init_helper\n",
      "    self.reducer = dist.Reducer(\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 2; 39.42 GiB total capacity; 37.86 GiB already allocated; 41.00 MiB free; 37.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/train.py\", line 240, in <module>\n",
      "    train()\n",
      "  File \"/opt/ml/code/train.py\", line 233, in train\n",
      "    trainer.train()\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1628, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1715, in _inner_training_loop\n",
      "    model = self._wrap_model(self.model_wrapped)\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1540, in _wrap_model\n",
      "    model = nn.parallel.DistributedDataParallel(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 688, in __init__\n",
      "    self._ddp_init_helper(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 825, in _ddp_init_helper\n",
      "    self.reducer = dist.Reducer(\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 1; 39.42 GiB total capacity; 37.86 GiB already allocated; 41.00 MiB free; 37.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/train.py\", line 240, in <module>\n",
      "    train()\n",
      "  File \"/opt/ml/code/train.py\", line 233, in train\n",
      "    trainer.train()\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1628, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1715, in _inner_training_loop\n",
      "    model = self._wrap_model(self.model_wrapped)\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1540, in _wrap_model\n",
      "    model = nn.parallel.DistributedDataParallel(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 688, in __init__\n",
      "    self._ddp_init_helper(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 825, in _ddp_init_helper\n",
      "    self.reducer = dist.Reducer(\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 4; 39.42 GiB total capacity; 37.86 GiB already allocated; 41.00 MiB free; 37.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/train.py\", line 240, in <module>\n",
      "    train()\n",
      "  File \"/opt/ml/code/train.py\", line 233, in train\n",
      "    trainer.train()\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1628, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1715, in _inner_training_loop\n",
      "    model = self._wrap_model(self.model_wrapped)\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1540, in _wrap_model\n",
      "    model = nn.parallel.DistributedDataParallel(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 688, in __init__\n",
      "    self._ddp_init_helper(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 825, in _ddp_init_helper\n",
      "    self.reducer = dist.Reducer(\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 6; 39.42 GiB total capacity; 37.86 GiB already allocated; 41.00 MiB free; 37.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/train.py\", line 240, in <module>\n",
      "    train()\n",
      "  File \"/opt/ml/code/train.py\", line 233, in train\n",
      "    trainer.train()\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1628, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1715, in _inner_training_loop\n",
      "    model = self._wrap_model(self.model_wrapped)\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1540, in _wrap_model\n",
      "    model = nn.parallel.DistributedDataParallel(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 688, in __init__\n",
      "    self._ddp_init_helper(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 825, in _ddp_init_helper\n",
      "    self.reducer = dist.Reducer(\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 5; 39.42 GiB total capacity; 37.86 GiB already allocated; 41.00 MiB free; 37.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/train.py\", line 240, in <module>\n",
      "    train()\n",
      "  File \"/opt/ml/code/train.py\", line 233, in train\n",
      "    trainer.train()\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1628, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1715, in _inner_training_loop\n",
      "    model = self._wrap_model(self.model_wrapped)\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1540, in _wrap_model\n",
      "    model = nn.parallel.DistributedDataParallel(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 688, in __init__\n",
      "    self._ddp_init_helper(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 825, in _ddp_init_helper\n",
      "    self.reducer = dist.Reducer(\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 7; 39.42 GiB total capacity; 38.03 GiB already allocated; 13.00 MiB free; 38.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/train.py\", line 240, in <module>\n",
      "    train()\n",
      "  File \"/opt/ml/code/train.py\", line 233, in train\n",
      "    trainer.train()\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1628, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1715, in _inner_training_loop\n",
      "    model = self._wrap_model(self.model_wrapped)\n",
      "  File \"/data/sm-alpaca/transformers-zphan/src/transformers/trainer.py\", line 1540, in _wrap_model\n",
      "    model = nn.parallel.DistributedDataParallel(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 688, in __init__\n",
      "    self._ddp_init_helper(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/nn/parallel/distributed.py\", line 825, in _ddp_init_helper\n",
      "    self.reducer = dist.Reducer(\u001b[0m\n",
      "\u001b[34mtorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 39.42 GiB total capacity; 38.03 GiB already allocated; 13.00 MiB free; 38.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\u001b[0m\n",
      "\u001b[34mERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 92) of binary: /usr/local/bin/python\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/distributed/run.py\", line 794, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
      "    raise ChildFailedError(\u001b[0m\n",
      "\u001b[34mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34mtrain.py FAILED\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mFailures:\u001b[0m\n",
      "\u001b[34m[1]:\n",
      "  time      : 2023-04-10_13:07:52\n",
      "  host      : algo-1\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 93)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[2]:\n",
      "  time      : 2023-04-10_13:07:52\n",
      "  host      : algo-1\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 94)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[3]:\n",
      "  time      : 2023-04-10_13:07:52\n",
      "  host      : algo-1\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 95)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[4]:\n",
      "  time      : 2023-04-10_13:07:52\n",
      "  host      : algo-1\n",
      "  rank      : 4 (local_rank: 4)\n",
      "  exitcode  : 1 (pid: 96)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[5]:\n",
      "  time      : 2023-04-10_13:07:52\n",
      "  host      : algo-1\n",
      "  rank      : 5 (local_rank: 5)\n",
      "  exitcode  : 1 (pid: 97)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[6]:\n",
      "  time      : 2023-04-10_13:07:52\n",
      "  host      : algo-1\n",
      "  rank      : 6 (local_rank: 6)\n",
      "  exitcode  : 1 (pid: 98)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[7]:\n",
      "  time      : 2023-04-10_13:07:52\n",
      "  host      : algo-1\n",
      "  rank      : 7 (local_rank: 7)\n",
      "  exitcode  : 1 (pid: 99)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mRoot Cause (first observed failure):\u001b[0m\n",
      "\u001b[34m[0]:\n",
      "  time      : 2023-04-10_13:07:52\n",
      "  host      : algo-1\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 92)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34m./train-llama7B.sh: 6: --output_dir: not found\u001b[0m\n",
      "\u001b[34m2023-04-10 13:07:52,530 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2023-04-10 13:07:52,530 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 127\u001b[0m\n",
      "\u001b[34mErrorMessage \"\"\u001b[0m\n",
      "\u001b[34mCommand \"/bin/sh -c ./train-llama7B.sh \"\u001b[0m\n",
      "\u001b[34m2023-04-10 13:07:52,530 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2023-04-10 13:08:08 Uploading - Uploading generated training model\n",
      "2023-04-10 13:08:08 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job nlp-sft-persona-7b-2023-04-10-12-58-28-129: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 127\nErrorMessage \"\"\nCommand \"/bin/sh -c ./train-llama7B.sh \", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpytorch_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_s3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# pytorch_estimator.fit()\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:284\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:1198\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/estimator.py:2344\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2342\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:4669\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   4666\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 4669\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   4671\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:4176\u001b[0m, in \u001b[0;36mSession._check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   4170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   4171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   4172\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   4173\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   4174\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   4175\u001b[0m     )\n\u001b[0;32m-> 4176\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   4177\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   4178\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   4179\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   4180\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job nlp-sft-persona-7b-2023-04-10-12-58-28-129: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 127\nErrorMessage \"\"\nCommand \"/bin/sh -c ./train-llama7B.sh \", exit code: 1"
     ]
    }
   ],
   "source": [
    "pytorch_estimator.fit(train_s3)\n",
    "# pytorch_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b8fecfb-3ae9-4e3f-aa18-e2cdd9271830",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp-alapca-125m-2023-03-24-04-03-18-624\n"
     ]
    }
   ],
   "source": [
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "def stop_training_job(name):\n",
    "    status = sm.describe_training_job(TrainingJobName=name)[\"TrainingJobStatus\"]\n",
    "    if status == \"InProgress\":\n",
    "        sm.stop_training_job(TrainingJobName=name)\n",
    "\n",
    "print(pytorch_estimator.latest_training_job.name)\n",
    "stop_training_job(pytorch_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced01fb-560e-48e6-b01f-fa535fe78f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
